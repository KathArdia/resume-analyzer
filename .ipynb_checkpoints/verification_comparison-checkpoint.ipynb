{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z8IOMi5KLzzF",
    "outputId": "c9825051-1196-4cb5-ca31-1026ec0bdcc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
      "Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
      "   ---------------------------------------- 0.0/7.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/7.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/7.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/7.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/7.1 MB 1.5 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.0/7.1 MB 2.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.8/7.1 MB 2.2 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.9/7.1 MB 3.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.1/7.1 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.4/7.1 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.7/7.1 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.7/7.1 MB 2.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.9/7.1 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.2/7.1 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.5/7.1 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 4.7/7.1 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.0/7.1 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.2/7.1 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.5/7.1 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.8/7.1 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.0/7.1 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.3/7.1 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.6/7.1 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.8/7.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.1/7.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.1/7.1 MB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.2\n",
      "Collecting textblob\n",
      "  Using cached textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Using cached textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.18.0.post0\n",
      "Requirement already satisfied: python-docx in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.0.1)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     -------------------- ----------------- 524.3/981.5 kB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 981.5/981.5 kB 1.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting language-tool-python\n",
      "  Downloading language_tool_python-2.8.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-docx) (5.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: six in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langdetect) (1.17.0)\n",
      "Requirement already satisfied: pip in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-tool-python) (24.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-tool-python) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-tool-python) (4.67.1)\n",
      "Requirement already satisfied: wheel in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-tool-python) (0.43.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->language-tool-python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->language-tool-python) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->language-tool-python) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->language-tool-python) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\athur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->language-tool-python) (0.4.6)\n",
      "Downloading language_tool_python-2.8.1-py3-none-any.whl (35 kB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993249 sha256=4a438c366928a3d81591ddd9823b41b23571b31e271d62b237255f5b657f42b4\n",
      "  Stored in directory: c:\\users\\athur\\appdata\\local\\pip\\cache\\wheels\\c1\\67\\88\\e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect, language-tool-python\n",
      "Successfully installed langdetect-1.0.9 language-tool-python-2.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker\n",
    "!pip install textblob\n",
    "!pip install python-docx PyPDF2 langdetect language-tool-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "59uD-iVOP7c3"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import docx\n",
    "import PyPDF2\n",
    "from langdetect import detect\n",
    "import language_tool_python\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEbOraJLP7rn"
   },
   "outputs": [],
   "source": [
    "######## проверяем этот кусок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "R6XiuFE-P7xa",
    "outputId": "4e402779-36db-4624-a81f-a060e23d578d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружается файл резюме (DOCX или PDF):\n",
      "Загружается файл с требованиями (JSON):\n",
      "\n",
      "--- Исправленное резюме ---\n",
      "\n",
      "Резюме: Data Analyst\n",
      "Иванов Иван \n",
      "\n",
      "Контактная информация:\n",
      "Email: ivanovivan@mai.com\n",
      "Телефон: +7 912 345 67 89\n",
      "\n",
      "\n",
      "\n",
      "Владею следующими технологиями: Big Data, Технические навыки:, Pandas, Анализ данных, Машинное обучение, Tableau, Python, Data Cleaning, Visualizations, SQL.\n",
      "Цель:\n",
      "Ищу вакансию на позицию Data Analyst, чтобы использовать свои аналитические и технические навыки для решения бизнес-зала и анализа данных.\n",
      "\n",
      "Образование:\n",
      "2015-2019: МГТУ им Баумана\n",
      "Специальность: Прикладная математика и информатика\n",
      "\n",
      "Опыт работы:\n",
      "Аналитик данных в компании \"Кутусовна\"\n",
      "Период работы: Январь 2020 — Сентябрь 2021\n",
      "Основные обязанности:\n",
      "Анализ больших данных.\n",
      "Создание отчетов с использованием SQL и Excel.\n",
      "Разработка визуализаций в Tableau.\n",
      "Аналитик данных в компании \"Дата\"\n",
      "Период работы: Февраль 2023 — Март 2023\n",
      "Основные обязанности:\n",
      "Анализ больших данных.\n",
      "Создание отчетов с использованием SQL и Excel.\n",
      "Разработка визуализаций в Tableau.\n",
      "\n",
      "Навыки:\n",
      "Технические навыки:\n",
      "Python, Pandas, Numpy\n",
      "SQL, Tableau\n",
      "Microsoft Excel\n",
      "Основы машинного обучения\n",
      "Дополнительные навыки:\n",
      "Анализ данных.\n",
      "Визуализация данных.\n",
      "\n",
      "Дополнительно:\n",
      "В 2022 году успешно завершил курсы по Data Science на платформе Coursera.\n",
      "Уровень английского: Intermediate.\n",
      "\n",
      "Сгенерированное резюме сохранено по пути: C:\\Users\\Athur\\resume-analyzer\\Исправленное_резюме.docx\n",
      "\n",
      "--- Замечания ---\n",
      "\n",
      "Ошибки орфографии и грамматики:\n",
      "Ошибка: Возможно найдена орфографическая ошибка., Позиция: 43\n",
      "Ошибка: Возможно найдена орфографическая ошибка., Позиция: 43\n",
      "\n",
      "Пропущенные ключевые слова:\n",
      "Машинное обучение, Big Data, Visualizations, Data Cleaning\n",
      "\n",
      "Соответствие требованиям: 55.56%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import docx\n",
    "import PyPDF2\n",
    "import language_tool_python\n",
    "from typing import List, Dict\n",
    "from langdetect import detect\n",
    "from datetime import datetime\n",
    "\n",
    "# Инициализация инструмента проверки орфографии\n",
    "language_tool = language_tool_python.LanguageTool('ru')\n",
    "\n",
    "# Чтение текста из DOCX файла\n",
    "def read_docx(file_path: str) -> str:\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# Чтение текста из PDF файла\n",
    "def read_pdf(file_path: str) -> str:\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Загрузка требований из файла JSON\n",
    "def load_requirements(file_path: str) -> List[str]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        requirements = json.load(file)\n",
    "    return requirements.get(\"job_keywords\", [])\n",
    "\n",
    "# Проверка орфографии и грамматики\n",
    "def check_spelling_and_grammar(text: str) -> Dict[str, List[str]]:\n",
    "    technology_keywords = [\"Java\", \"Python\", \"SQL\", \"C++\", \"JavaScript\", \"HTML\", \"CSS\"]\n",
    "    matches = language_tool.check(text)\n",
    "    errors = []\n",
    "    error_words = []\n",
    "\n",
    "    for match in matches:\n",
    "        error_fragment = text[match.offset:match.offset + match.errorLength]\n",
    "        # Пропускаем ключевые слова технологий\n",
    "        if not any(tech.lower() in error_fragment.lower() for tech in technology_keywords):\n",
    "            errors.append(f\"Ошибка: {match.message}, Позиция: {match.offsetInContext}\")\n",
    "            error_words.append(error_fragment.strip())  # Добавляем само слово с ошибкой\n",
    "\n",
    "    return {\n",
    "        \"errors\": errors,\n",
    "        \"error_words\": list(set(error_words))  # Убираем дублирования слов\n",
    "    }\n",
    "\n",
    "# Проверка ключевых слов\n",
    "def check_keywords(resume: str, job_keywords: List[str]) -> Dict[str, List[str]]:\n",
    "    found_keywords = [kw for kw in job_keywords if kw.lower() in resume.lower()]\n",
    "    missing_keywords = list(set(job_keywords) - set(found_keywords))\n",
    "    return {\n",
    "        \"found_keywords\": found_keywords,\n",
    "        \"missing_keywords\": missing_keywords,\n",
    "    }\n",
    "\n",
    "# Проверка хронологии\n",
    "def check_timing(resume: str) -> List[str]:\n",
    "    date_range_pattern = re.compile(\n",
    "        r'(?P<start>\\b\\d{1,2}\\s+[А-яёЁ]+\\s+\\d{4}|\\b[А-яёЁ]+\\s+\\d{4}|\\d{4})\\s*-\\s*(?P<end>\\b\\d{1,2}\\s+[А-яёЁ]+\\s+\\d{4}|\\b[А-яёЁ]+\\s+\\d{4}|\\d{4})'\n",
    "    )\n",
    "    months = {\n",
    "        \"январь\": 1, \"февраль\": 2, \"март\": 3, \"апрель\": 4, \"май\": 5, \"июнь\": 6,\n",
    "        \"июль\": 7, \"август\": 8, \"сентябрь\": 9, \"октябрь\": 10, \"ноябрь\": 11, \"декабрь\": 12\n",
    "    }\n",
    "\n",
    "    def parse_date(date_str: str) -> datetime:\n",
    "        try:\n",
    "            date_str = date_str.strip()\n",
    "            if re.match(r'\\d{1,2}\\s+[А-яёЁ]+\\s+\\d{4}', date_str):\n",
    "                day, month_text, year = date_str.split()\n",
    "                return datetime(int(year), months[month_text.lower()], int(day))\n",
    "            elif re.match(r'[А-яёЁ]+\\s+\\d{4}', date_str):\n",
    "                month_text, year = date_str.split()\n",
    "                return datetime(int(year), months[month_text.lower()], 1)\n",
    "            elif re.match(r'\\d{4}', date_str):\n",
    "                return datetime(int(date_str), 1, 1)\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Ошибка: Неизвестный месяц '{month_text}' в дате '{date_str}'\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Ошибка при обработке даты '{date_str}': {e}\")\n",
    "\n",
    "    periods = []\n",
    "    for match in date_range_pattern.finditer(resume):\n",
    "        start_date = parse_date(match.group('start'))\n",
    "        end_date = parse_date(match.group('end'))\n",
    "        periods.append((start_date, end_date))\n",
    "\n",
    "    periods.sort(key=lambda x: x[0])\n",
    "\n",
    "    issues = []\n",
    "    for i in range(len(periods) - 1):\n",
    "        current_end = periods[i][1]\n",
    "        next_start = periods[i + 1][0]\n",
    "        gap = (next_start - current_end).days\n",
    "        if gap > 183:\n",
    "            issues.append(\n",
    "                f\"Обнаружен перерыв в хронологии более полугода между {current_end.strftime('%d.%m.%Y')} \"\n",
    "                f\"и {next_start.strftime('%d.%m.%Y')}.\"\n",
    "            )\n",
    "\n",
    "    return issues\n",
    "\n",
    "# Генерация суммаризации\n",
    "def generate_summary(skills: List[str], job_keywords: List[str]) -> str:\n",
    "    combined_skills = list(set(skills + job_keywords))\n",
    "    summary = f\"Владею следующими технологиями: {', '.join(combined_skills)}.\"\n",
    "    return summary\n",
    "\n",
    "# Вставка суммаризации после телефона\n",
    "def insert_summary_into_corrected_text(corrected_text: str, summary: str) -> str:\n",
    "    phone_pattern = re.compile(r\"(Телефон[:\\-]?\\s*\\+?\\d[\\d\\s\\-\\(\\)]*)\", re.IGNORECASE)\n",
    "    match = phone_pattern.search(corrected_text)\n",
    "\n",
    "    if match:\n",
    "        phone_section = match.group(0)\n",
    "        summary_text = f\"\\n\\n{summary}\\n\"\n",
    "        return corrected_text.replace(phone_section, phone_section + summary_text, 1)\n",
    "    return corrected_text + f\"\\n\\n{summary}\"\n",
    "\n",
    "# Формирование улучшенного текста\n",
    "def generate_corrected_text(text: str) -> str:\n",
    "    matches = language_tool.check(text)\n",
    "    corrected_text = text\n",
    "    technology_keywords = [\"Java\", \"Python\", \"SQL\", \"C++\", \"JavaScript\", \"HTML\", \"CSS\"]\n",
    "    for match in matches:\n",
    "        error_fragment = text[match.offset:match.offset + match.errorLength]\n",
    "        if not any(tech.lower() in error_fragment.lower() for tech in technology_keywords):\n",
    "            corrected_text = language_tool.correct(corrected_text)\n",
    "    return corrected_text\n",
    "\n",
    "# Анализ резюме\n",
    "def analyze_resume(file_path: str, job_keywords: List[str]) -> Dict:\n",
    "    if file_path.endswith('.docx'):\n",
    "        text = read_docx(file_path)\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        text = read_pdf(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Формат файла не поддерживается. Используйте DOCX или PDF.\")\n",
    "\n",
    "    if detect(text) != 'ru':\n",
    "        raise ValueError(\"Язык резюме должен быть русским.\")\n",
    "\n",
    "    spelling_and_grammar_errors = check_spelling_and_grammar(text)\n",
    "    keyword_check = check_keywords(text, job_keywords)\n",
    "    timing_issues = check_timing(text)\n",
    "\n",
    "    skills_section = re.search(r\"Навыки:\\s*(.*)\", text, re.IGNORECASE)\n",
    "    skills = [skill.strip() for skill in skills_section.group(1).split(\",\")] if skills_section else []\n",
    "\n",
    "    corrected_text = generate_corrected_text(text)\n",
    "    summary = generate_summary(skills, job_keywords)\n",
    "    corrected_text_with_summary = insert_summary_into_corrected_text(corrected_text, summary)\n",
    "\n",
    "    match_score = len(keyword_check[\"found_keywords\"]) / len(job_keywords) * 100 if job_keywords else 0\n",
    "    return {\n",
    "        \"original_text\": text,\n",
    "        \"corrected_text\": corrected_text_with_summary,\n",
    "        \"summary\": summary,\n",
    "        \"spelling_and_grammar_errors\": spelling_and_grammar_errors,\n",
    "        \"missing_keywords\": keyword_check[\"missing_keywords\"],\n",
    "        \"timing_issues\": timing_issues,\n",
    "        \"match_score\": match_score\n",
    "    }\n",
    "\n",
    "# Функция для сохранения исправленного резюме\n",
    "def save_resume_to_word(resume_text: str, file_name: str):\n",
    "    output_dir = r\"C:\\Users\\Athur\\resume-analyzer\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    file_path = os.path.join(output_dir, file_name)  # Убедимся, что используется правильный путь\n",
    "    doc = docx.Document()\n",
    "    for line in resume_text.split(\"\\n\"):\n",
    "        doc.add_paragraph(line)\n",
    "    doc.save(file_path)\n",
    "\n",
    "    print(f\"Сгенерированное резюме сохранено по пути: {file_path}\")\n",
    "\n",
    "# Основной блок кода\n",
    "if __name__ == \"__main__\":\n",
    "    # Указываем путь к файлу резюме\n",
    "    print(\"Загружается файл резюме (DOCX или PDF):\")\n",
    "    uploaded = {\"C:\\\\Users\\\\Athur\\\\resume-analyzer\\\\resume.docx\": \"resume.docx\"}  # Симулируем загрузку файла\n",
    "    resume_file_path = list(uploaded.keys())[0]\n",
    "\n",
    "    # Указываем путь к файлу с требованиями\n",
    "    print(\"Загружается файл с требованиями (JSON):\")\n",
    "    uploaded_requirements = {\"C:\\\\Users\\\\Athur\\\\resume-analyzer\\\\notebooks\\\\requirements.json\": \"requirements.json\"}  # Симулируем загрузку файла\n",
    "    requirements_file_path = list(uploaded_requirements.keys())[0]\n",
    "\n",
    "    # Загрузка ключевых слов из JSON\n",
    "    with open(requirements_file_path, 'r', encoding='utf-8') as file:\n",
    "        job_keywords = json.load(file).get(\"job_keywords\", [])\n",
    "\n",
    "    # Анализ резюме\n",
    "    try:\n",
    "        result = analyze_resume(resume_file_path, job_keywords)\n",
    "\n",
    "        print(\"\\n--- Исправленное резюме ---\\n\")\n",
    "        print(result[\"corrected_text\"])\n",
    "\n",
    "        # Сохранение исправленного резюме\n",
    "        save_resume_to_word(result[\"corrected_text\"], \"Исправленное_резюме.docx\")\n",
    "\n",
    "        print(\"\\n--- Замечания ---\\n\")\n",
    "        print(\"Ошибки орфографии и грамматики:\")\n",
    "        if result[\"spelling_and_grammar_errors\"][\"errors\"]:\n",
    "            print(\"\\n\".join(result[\"spelling_and_grammar_errors\"][\"errors\"]))\n",
    "        else:\n",
    "            print(\"Ошибок не найдено.\")\n",
    "\n",
    "        print(\"\\nПропущенные ключевые слова:\")\n",
    "        print(\", \".join(result[\"missing_keywords\"]) if result[\"missing_keywords\"] else \"Все ключевые слова найдены.\")\n",
    "\n",
    "        print(f\"\\nСоответствие требованиям: {result['match_score']:.2f}%\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Ошибка: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
